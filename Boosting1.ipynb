{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb8f5b-0245-42c8-b82c-aeb761e5e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1)\n",
    "Boosting is a machine learning ensemble technique that aims to improve the predictive performance of a model by combining the strengths of multiple weak learners. A weak learner is a model that performs slightly better than random chance. Boosting involves training a sequence of weak learners, where each subsequent model focuses on correcting the errors of the previous ones.\n",
    "\n",
    "The general boosting process works as follows:\n",
    "\n",
    "1. **Train a Weak Learner:** Start by training a weak model on the original dataset.\n",
    "\n",
    "2. **Weighted Data:** Adjust the weights of the misclassified instances in the training set, giving more emphasis to the data points that the model struggled to classify correctly.\n",
    "\n",
    "3. **Train a New Weak Learner:** Train a new weak learner on the modified dataset, placing more importance on the previously misclassified instances.\n",
    "\n",
    "4. **Combine Models:** Combine the predictions of all weak learners, often using a weighted sum. The combined model is often more accurate than any individual weak learner.\n",
    "\n",
    "5. **Repeat:** Steps 2-4 are repeated for a specified number of iterations or until a certain level of accuracy is achieved.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost. These algorithms differ in their specific strategies for adjusting weights and combining weak learners, but they share the common goal of improving overall model performance through iterative learning. Boosting is effective for a wide range of tasks, including classification and regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec528df-8293-476a-98cb-8b2213fb7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2)\n",
    "**Advantages of Boosting Techniques:**\n",
    "\n",
    "1. **Improved Accuracy:** Boosting often leads to higher accuracy compared to individual weak learners, as it focuses on correcting errors and refining the model iteratively.\n",
    "\n",
    "2. **Versatility:** Boosting algorithms can be applied to various types of machine learning tasks, including classification and regression, making them versatile for different problems.\n",
    "\n",
    "3. **Handles Complex Relationships:** Boosting can capture complex relationships in the data, allowing it to learn intricate patterns and improve model performance.\n",
    "\n",
    "4. **Reduction of Overfitting:** Boosting helps reduce overfitting by iteratively adjusting the model to minimize errors. This is particularly beneficial when dealing with noisy data.\n",
    "\n",
    "5. **Interpretability:** Boosting algorithms can provide insights into feature importance, helping users understand which features contribute more to the model's predictions.\n",
    "\n",
    "**Limitations of Boosting Techniques:**\n",
    "\n",
    "1. **Sensitivity to Noisy Data:** Boosting can be sensitive to noisy data and outliers, as it may try to correct errors introduced by these instances during the iterative process.\n",
    "\n",
    "2. **Computational Complexity:** Training boosting models can be computationally intensive and time-consuming, especially for large datasets or complex weak learners.\n",
    "\n",
    "3. **Overemphasis on Outliers:** In some cases, boosting may give too much emphasis to misclassified instances, leading to overfitting and reduced generalization performance.\n",
    "\n",
    "4. **Need for Tuning:** The performance of boosting models can depend on hyperparameter settings, and finding the optimal values may require extensive tuning.\n",
    "\n",
    "5. **Potential for Bias:** If the weak learners are biased in a particular direction, boosting may amplify this bias in the final combined model.\n",
    "\n",
    "Despite these limitations, boosting remains a powerful and widely used ensemble technique in machine learning, providing significant improvements in predictive performance when applied appropriately. It's important to carefully consider the characteristics of the data and choose hyperparameters judiciously to achieve the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b65105-8f7f-443e-a44a-5cd7c33d4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3)\n",
    "Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong predictive model. The general idea behind boosting can be explained in several steps:\n",
    "\n",
    "1. **Initialize Weights:** Assign equal weights to all data points in the training set. Initially, each data point has an equal influence on the model.\n",
    "\n",
    "2. **Train a Weak Learner:** Start by training a weak learner (a model that performs slightly better than random chance) on the original dataset. The weak learner is typically a simple model, such as a decision tree with limited depth.\n",
    "\n",
    "3. **Evaluate and Update Weights:** Evaluate the performance of the weak learner on the training set. Increase the weights of misclassified data points, so they have more influence on the next iteration. This ensures that the next weak learner focuses more on the previously misclassified instances.\n",
    "\n",
    "4. **Repeat the Process:** Train a new weak learner on the updated dataset, which gives more importance to the misclassified instances. Repeat this process iteratively for a predefined number of rounds or until a certain level of accuracy is reached.\n",
    "\n",
    "5. **Combine Weak Learners:** Combine the predictions of all weak learners, often using a weighted sum or voting mechanism. The combined model, also known as the strong learner, is more accurate than any individual weak learner.\n",
    "\n",
    "The intuition behind boosting is that each weak learner focuses on the mistakes of the previous ones, gradually improving the overall model's performance. The final model is a weighted combination of these weak learners, where the weights are determined based on their individual accuracies.\n",
    "\n",
    "Common boosting algorithms include:\n",
    "\n",
    "- **AdaBoost (Adaptive Boosting):** Adjusts weights on data points to emphasize misclassified instances.\n",
    "  \n",
    "- **Gradient Boosting:** Builds trees sequentially, with each tree trying to correct the errors of the previous one by fitting to the residuals.\n",
    "\n",
    "- **XGBoost (Extreme Gradient Boosting):** An optimized version of gradient boosting that includes regularization terms, parallel processing, and other enhancements for better performance.\n",
    "\n",
    "Boosting is effective in improving accuracy, handling complex relationships in data, and reducing overfitting. However, it is sensitive to noisy data, and care must be taken to tune hyperparameters appropriately for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0697e0-719f-4698-a8c4-8b3cd4238a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4)\n",
    "There are several boosting algorithms, each with its own variations and characteristics. Some of the prominent boosting algorithms include:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):** AdaBoost is one of the earliest and most well-known boosting algorithms. It assigns weights to data points and adjusts them during each iteration to emphasize the misclassified instances. The final prediction is a weighted combination of weak learners.\n",
    "\n",
    "2. **Gradient Boosting Machines (GBM):** Gradient Boosting builds trees sequentially, with each tree fitting to the residuals (the differences between the actual and predicted values) of the previous trees. It minimizes the loss function, such as mean squared error in regression problems or cross-entropy in classification problems.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting):** XGBoost is an optimized and scalable version of gradient boosting. It incorporates additional features such as parallel processing, regularization terms, and a second-order optimization method. XGBoost has gained popularity for its efficiency and high performance in various machine learning competitions.\n",
    "\n",
    "4. **LightGBM:** Similar to XGBoost, LightGBM is a gradient boosting framework that is designed for speed and efficiency. It uses a histogram-based approach to split data during the tree-building process, reducing the computational complexity and making it suitable for large datasets.\n",
    "\n",
    "5. **CatBoost:** CatBoost is a boosting algorithm that is particularly effective for categorical feature handling. It automatically handles categorical variables without the need for preprocessing and is designed to be resistant to overfitting.\n",
    "\n",
    "6. **Stochastic Gradient Boosting (SGD):** Stochastic Gradient Boosting is a variant of gradient boosting that introduces randomness by using a subset of the training data and features at each iteration. This can help prevent overfitting and speed up the training process.\n",
    "\n",
    "7. **LogitBoost:** LogitBoost is a boosting algorithm specifically designed for binary classification problems. It minimizes logistic loss during training and combines weak learners to improve classification performance.\n",
    "\n",
    "8. **GBDT (Gradient Boosted Decision Trees):** GBDT is a general term for gradient boosting algorithms that use decision trees as weak learners. It encompasses algorithms like AdaBoost, XGBoost, and LightGBM.\n",
    "\n",
    "These boosting algorithms have been widely used in various machine learning applications and competitions, and the choice of algorithm often depends on the specific characteristics of the dataset and the problem at hand. Each algorithm has its strengths, and researchers and practitioners may experiment with different boosting techniques to find the most suitable one for a particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0039a-d644-4d66-8d96-88831731ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5)\n",
    "Boosting algorithms come with a set of parameters that can be tuned to optimize the performance of the model. The specific parameters may vary depending on the algorithm, but here are some common parameters found in many boosting algorithms:\n",
    "\n",
    "1. **Number of Trees (n_estimators):** The number of weak learners (trees) to be built during the boosting process. Increasing the number of trees can improve performance, but it also increases computation time.\n",
    "\n",
    "2. **Learning Rate (or Shrinkage):** A factor that scales the contribution of each weak learner. A smaller learning rate requires more iterations but can lead to better generalization.\n",
    "\n",
    "3. **Max Depth (max_depth):** The maximum depth of each tree. It controls the complexity of the weak learners. Deeper trees can capture more complex relationships but may also lead to overfitting.\n",
    "\n",
    "4. **Subsample:** The fraction of the training data used to fit each weak learner. It introduces randomness by considering only a subset of the data, which can prevent overfitting.\n",
    "\n",
    "5. **Colsample Bytree/Colsample Bylevel/Colsample Bynode:** These parameters control the fraction of features randomly chosen to grow each tree or level of a tree. They add an element of randomness and can improve generalization.\n",
    "\n",
    "6. **Min Child Weight (min_child_weight):** The minimum sum of instance weight (hessian) needed in a child. It is used to control overfitting by imposing a minimum requirement for the number of instances in each leaf node.\n",
    "\n",
    "7. **Gamma (min_split_loss):** A regularization term that specifies the minimum loss reduction required to make a further partition on a leaf node. It helps control tree growth and prevent overfitting.\n",
    "\n",
    "8. **Reg_alpha (L1 regularization) and Reg_lambda (L2 regularization):** Regularization terms that penalize large coefficients in the model. They help prevent overfitting by discouraging overly complex models.\n",
    "\n",
    "9. **Scale Pos Weight:** A parameter used in binary classification problems that helps balance the positive and negative weights, particularly when the classes are imbalanced.\n",
    "\n",
    "10. **Objective Function:** The loss function to be minimized during the training process. It depends on the specific task (e.g., regression, binary or multiclass classification) and can be customized based on the application.\n",
    "\n",
    "11. **Early Stopping:** A technique where training is stopped when the performance on a validation set stops improving. It helps prevent overfitting and reduces computation time.\n",
    "\n",
    "These parameters are crucial for fine-tuning the boosting model's performance, and their optimal values depend on the characteristics of the data and the specific problem being addressed. Grid search or random search can be employed to explore different combinations of parameter values to find the best configuration for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e995e7e-82db-4816-9310-1b473765c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6)\n",
    "Boosting algorithms combine weak learners to create a strong learner through a weighted sum or a voting mechanism. The general process involves iteratively training weak learners, adjusting their weights, and combining their predictions. Here's an overview of how boosting algorithms combine weak learners to form a strong learner:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Assign equal weights to all data points in the training set.\n",
    "   - Train the first weak learner on the original dataset.\n",
    "\n",
    "2. **Weighted Training:**\n",
    "   - Evaluate the performance of the weak learner.\n",
    "   - Adjust the weights of misclassified data points. Increase the weights of misclassified instances to give them more influence in the subsequent iterations.\n",
    "\n",
    "3. **Train a New Weak Learner:**\n",
    "   - Train a new weak learner on the updated dataset, where the weights emphasize the previously misclassified instances.\n",
    "   - Repeat this process for a predefined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "4. **Combine Predictions:**\n",
    "   - Assign weights to the weak learners based on their individual accuracies or errors. The more accurate a weak learner is, the higher its weight in the final combination.\n",
    "   - Combine the predictions of all weak learners using a weighted sum for regression problems or a weighted voting scheme for classification problems.\n",
    "\n",
    "   For regression:\n",
    "   \\[ \\text{Final Prediction} = \\sum_{i=1}^{N} \\alpha_i \\cdot \\text{WeakLearner}_i(x) \\]\n",
    "   \n",
    "   For binary classification:\n",
    "   \\[ \\text{Final Prediction} = \\text{sign}\\left(\\sum_{i=1}^{N} \\alpha_i \\cdot \\text{WeakLearner}_i(x)\\right) \\]\n",
    "\n",
    "   For multiclass classification, a weighted voting scheme is applied, where each class receives votes based on the weighted sum of predictions.\n",
    "\n",
    "5. **Output Final Prediction:**\n",
    "   - The final model, also known as the strong learner, is the combination of all weak learners.\n",
    "   - The strong learner's prediction is used as the final output for new, unseen data.\n",
    "\n",
    "The weights (\\(\\alpha_i\\)) assigned to each weak learner are determined based on their individual performance, with more accurate learners receiving higher weights. The combination of weak learners in boosting aims to correct errors made by the previous models, leading to a more accurate and robust model overall.\n",
    "\n",
    "The specific implementation details may vary between boosting algorithms, but the fundamental idea of combining weak learners through a weighted sum or voting mechanism remains consistent across different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f29f1-e428-4085-8fe4-d0d599dd778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7)\n",
    "AdaBoost, short for Adaptive Boosting, is a popular boosting algorithm that focuses on iteratively improving the performance of a model by giving more weight to misclassified instances. The key idea behind AdaBoost is to combine the predictions of multiple weak learners (usually shallow decision trees) to create a strong learner. Here's an overview of how the AdaBoost algorithm works:\n",
    "\n",
    "1. **Initialize Weights:**\n",
    "   - Assign equal weights to all data points in the training set. Initially, each data point has an equal influence on the model.\n",
    "\n",
    "2. **Train Weak Learner:**\n",
    "   - Train a weak learner (e.g., a decision tree with limited depth) on the original dataset.\n",
    "\n",
    "3. **Evaluate and Update Weights:**\n",
    "   - Evaluate the performance of the weak learner on the training set.\n",
    "   - Increase the weights of misclassified data points. This gives more emphasis to the instances that the weak learner struggled to classify correctly.\n",
    "\n",
    "4. **Compute Weak Learner Weight (\\(\\alpha\\)):**\n",
    "   - Calculate the weight (\\(\\alpha\\)) of the weak learner based on its accuracy. The formula for \\(\\alpha\\) is often:\n",
    "     \\[ \\alpha = \\frac{1}{2} \\ln\\left(\\frac{1 - \\text{error}}{\\text{error}}\\right) \\]\n",
    "   - The weight \\(\\alpha\\) is higher for more accurate weak learners and lower for less accurate ones.\n",
    "\n",
    "5. **Update Weights and Normalize:**\n",
    "   - Update the weights of the data points based on their classification by the weak learner. Misclassified instances receive higher weights.\n",
    "   - Normalize the weights so that they sum to 1.\n",
    "\n",
    "6. **Repeat:**\n",
    "   - Repeat steps 2-5 for a predefined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "7. **Combine Weak Learners:**\n",
    "   - Combine the weak learners by creating a weighted sum of their predictions. The final prediction is determined by the weighted sum of weak learners:\n",
    "     \\[ \\text{Final Prediction} = \\text{sign}\\left(\\sum_{i=1}^{N} \\alpha_i \\cdot \\text{WeakLearner}_i(x)\\right) \\]\n",
    "   - The sign function ensures that the final prediction is binary in the case of binary classification.\n",
    "\n",
    "The AdaBoost algorithm gives more influence to weak learners that perform well and less influence to those that perform poorly. By iteratively adjusting the weights and combining weak learners, AdaBoost creates a strong learner that is capable of achieving high accuracy even with simple base models. AdaBoost is effective in boosting the performance of various machine learning tasks, and its versatility makes it a widely used algorithm in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16827d6-9705-4ec9-8974-b3d023f2027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8)\n",
    "In AdaBoost, the loss function used is an exponential loss function, also known as the AdaBoost loss function. The goal of AdaBoost is to minimize this exponential loss during the training process. The exponential loss function is defined as follows:\n",
    "\n",
    "\\[ L(y, f(x)) = \\exp(-y \\cdot f(x)) \\]\n",
    "\n",
    "Where:\n",
    "- \\(y\\) is the true label of the instance (\\(y \\in \\{-1, +1\\}\\) for binary classification, and \\(y \\in \\{1, 2, \\ldots, K\\}\\) for multiclass classification),\n",
    "- \\(f(x)\\) is the weighted sum of weak learners' predictions, and\n",
    "- The exponential function \\(\\exp(\\cdot)\\) is used to emphasize the errors made by the weak learners.\n",
    "\n",
    "The goal during the training process is to find the weak learner that minimizes the weighted sum of this exponential loss across all instances. Specifically, the weak learner is chosen to minimize the weighted sum of the exponential loss over all data points:\n",
    "\n",
    "\\[ \\text{error} = \\sum_{i=1}^{N} w_i \\cdot \\exp(-y_i \\cdot f_i(x)) \\]\n",
    "\n",
    "Where:\n",
    "- \\(N\\) is the number of instances in the training set,\n",
    "- \\(w_i\\) is the weight assigned to the \\(i\\)-th instance (initially set to be equal for all instances),\n",
    "- \\(y_i\\) is the true label of the \\(i\\)-th instance, and\n",
    "- \\(f_i(x)\\) is the prediction of the weak learner for the \\(i\\)-th instance.\n",
    "\n",
    "The weak learner that minimizes this error is then assigned a weight (\\(\\alpha\\)), and the weights of misclassified instances are increased, creating an emphasis on the mistakes for the next iteration. This process is repeated for a predefined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "The exponential loss function in AdaBoost puts more emphasis on instances that are misclassified by the weak learners, guiding the algorithm to focus on correcting errors and improving the overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffbb1c-f917-4f15-9cd4-b144bdc97f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 9)\n",
    "In the AdaBoost algorithm, the weights of misclassified samples are updated to give more emphasis to those instances that the current weak learner struggled to classify correctly. The update is designed to ensure that the next weak learner focuses more on the previously misclassified samples. Here's a step-by-step explanation of how AdaBoost updates the weights:\n",
    "\n",
    "1. **Initialize Weights:**\n",
    "   - Assign equal weights to all data points in the training set. Initially, each data point has an equal influence on the model.\n",
    "\n",
    "2. **Train Weak Learner:**\n",
    "   - Train a weak learner (e.g., a decision tree with limited depth) on the original dataset.\n",
    "\n",
    "3. **Evaluate and Update Weights:**\n",
    "   - Evaluate the performance of the weak learner on the training set.\n",
    "   - Update the weights of the data points based on their classification by the weak learner.\n",
    "\n",
    "   For a binary classification problem, the weight update is given by:\n",
    "   \\[ w_i^{(t+1)} = w_i^{(t)} \\cdot \\exp(-\\alpha_t \\cdot y_i \\cdot h_t(x_i)) \\]\n",
    "\n",
    "   - \\(w_i^{(t)}\\) is the weight of the \\(i\\)-th instance before the update.\n",
    "   - \\(\\alpha_t\\) is the weight assigned to the weak learner at iteration \\(t\\).\n",
    "   - \\(y_i\\) is the true label of the \\(i\\)-th instance.\n",
    "   - \\(h_t(x_i)\\) is the prediction of the weak learner for the \\(i\\)-th instance.\n",
    "\n",
    "   This update increases the weights of misclassified instances, making them more influential in the subsequent iterations.\n",
    "\n",
    "4. **Normalize Weights:**\n",
    "   - Normalize the weights so that they sum to 1, ensuring that the weights remain valid probability distributions.\n",
    "\n",
    "   \\[ w_i^{(t+1)} = \\frac{w_i^{(t+1)}}{\\sum_{j=1}^{N} w_j^{(t+1)}} \\]\n",
    "\n",
    "   This normalization step prevents the weights from growing too large or too small over time.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Repeat steps 2-4 for a predefined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "By updating the weights of misclassified samples in each iteration, AdaBoost effectively guides the subsequent weak learners to focus more on the instances that were challenging to classify correctly in the previous iterations. The combination of these weighted weak learners results in a strong learner that is robust and capable of achieving high accuracy even with simple base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095b187-6a05-4b6d-8915-fa22b824831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 10)\n",
    "In the AdaBoost algorithm, the number of estimators refers to the total number of weak learners (usually decision trees) trained during the boosting process. Increasing the number of estimators can have both positive and negative effects, and the impact may vary depending on the characteristics of the data and the specific problem being addressed. Here are the effects of increasing the number of estimators in the AdaBoost algorithm:\n",
    "\n",
    "**Positive Effects:**\n",
    "\n",
    "1. **Improved Training Performance:** As the number of estimators increases, AdaBoost has more opportunities to correct errors and fit the training data better. This can lead to improved performance on the training set, resulting in a more accurate model.\n",
    "\n",
    "2. **Better Generalization:** A larger number of estimators allows AdaBoost to capture more complex patterns and relationships in the data. This can enhance the model's ability to generalize well to unseen data, leading to improved performance on the test set or in real-world scenarios.\n",
    "\n",
    "3. **Reduced Overfitting:** AdaBoost is less prone to overfitting compared to some other algorithms, and increasing the number of estimators can further contribute to reducing overfitting. The iterative nature of AdaBoost, combined with the weighting of misclassified instances, helps create a model that generalizes well.\n",
    "\n",
    "**Negative Effects:**\n",
    "\n",
    "1. **Increased Computational Cost:** Training more weak learners requires additional computational resources and time. The algorithm becomes more computationally expensive as the number of estimators grows, especially for large datasets or complex weak learners.\n",
    "\n",
    "2. **Diminishing Returns:** After a certain point, the additional benefit gained by adding more weak learners may diminish. The marginal improvement in model performance may not justify the computational cost and time required for training.\n",
    "\n",
    "3. **Potential for Overfitting:** While AdaBoost is less prone to overfitting compared to some other algorithms, excessively increasing the number of estimators could lead to overfitting, especially if the weak learners become too complex.\n",
    "\n",
    "4. **Increased Sensitivity to Noisy Data:** If the dataset contains noise or outliers, increasing the number of estimators may lead AdaBoost to overemphasize these instances, potentially affecting the model's robustness.\n",
    "\n",
    "**Guidelines:**\n",
    "\n",
    "- It's often recommended to monitor the performance on a validation set and stop increasing the number of estimators when the performance saturates or starts to degrade.\n",
    "  \n",
    "- Cross-validation can help determine an optimal number of estimators by assessing performance on different subsets of the training data.\n",
    "\n",
    "- The choice of the number of estimators should consider a trade-off between model performance and computational resources available.\n",
    "\n",
    "In summary, increasing the number of estimators in the AdaBoost algorithm can enhance model performance up to a certain point, but careful consideration is needed to avoid diminishing returns, overfitting, and increased computational costs. Experimenting with different values and monitoring performance on validation sets is essential to finding the right balance for a given problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
