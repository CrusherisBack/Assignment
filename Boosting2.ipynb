{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc6fc2-b05e-4890-b8ca-274e49a0cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1)\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, which involves predicting a continuous output variable. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model.\n",
    "\n",
    "Here's a brief overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization:** The algorithm starts with an initial prediction, usually the mean or median of the target variable.\n",
    "\n",
    "2. **Building Trees (Weak Learners):** A series of weak learners, often decision trees, are sequentially added to the model. Each tree is trained to correct the errors made by the combination of the existing trees.\n",
    "\n",
    "3. **Weighted Combination:** The predictions of all the weak learners are combined, and each tree's contribution is weighted based on its performance and the error it helps reduce. The weights are determined through a process of gradient descent, where the algorithm minimizes the residual errors.\n",
    "\n",
    "4. **Iterative Process:** The process of adding trees and updating weights is repeated for a predefined number of iterations or until a specified level of performance is achieved.\n",
    "\n",
    "5. **Final Prediction:** The final prediction is the sum of the initial prediction and the contributions from all the weak learners.\n",
    "\n",
    "Gradient Boosting Regression is known for its high predictive accuracy and robustness. Popular implementations of Gradient Boosting Regression include XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor. These implementations often include additional optimizations and features to enhance performance and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae06697d-84a6-4076-bca7-3e47ef7c7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
      "/tmp/ipykernel_110/3512682308.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 95.13779746436367\n",
      "R-squared: -1.5931197810079305\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer 2)\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target variable\n",
    "        y_pred = np.full_like(y, np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Make predictions with the current tree and update the ensemble\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the predictions from all trees\n",
    "        y_pred = np.sum(self.learning_rate * tree.predict(X) for tree in self.models)\n",
    "        return y_pred\n",
    "\n",
    "# Create and train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = np.array([gb_model.predict(np.array([x])) for x in X_test])\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test, y_test, label='Actual')\n",
    "plt.scatter(X_test, y_pred, label='Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fd07d-4248-4831-9258-cce0846ad4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = np.array([best_model.predict(np.array([x])) for x in X_test])\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Mean Squared Error: {mse_best}\")\n",
    "print(f\"Best R-squared: {r2_best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8696d-9265-4428-be3c-051756b966ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4)\n",
    "In the context of Gradient Boosting, a weak learner refers to a model that performs slightly better than random chance on a particular task. In the case of regression problems, weak learners are typically shallow decision trees. These are decision trees with limited depth, often referred to as \"stumps\" when they consist of just one node and two leaves.\n",
    "\n",
    "The key characteristic of a weak learner is its limited predictive power and complexity. Weak learners are not expected to make accurate predictions on their own, but they contribute incrementally to the overall predictive power of the ensemble when combined with other weak learners.\n",
    "\n",
    "In the context of Gradient Boosting, the algorithm builds a series of weak learners sequentially, with each new learner focusing on correcting the errors made by the combined predictions of the existing ensemble. The process involves fitting a weak learner to the residuals (the differences between the predicted and true values) of the current ensemble. By doing so, each weak learner improves the overall model by addressing the mistakes of its predecessors.\n",
    "\n",
    "While decision trees are commonly used as weak learners, other models like linear regression models or shallow neural networks can also serve this purpose. The choice of weak learner may depend on the specific characteristics of the dataset and the problem at hand. The success of Gradient Boosting relies on the ability of weak learners to capture and correct the remaining errors in the predictions, gradually improving the overall model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ece840-d8fc-4bd2-9c3b-4a2441bcba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5)\n",
    "The Gradient Boosting algorithm is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong predictive model. The intuition behind Gradient Boosting can be explained in the following steps:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The algorithm starts with an initial prediction for the target variable. This initial prediction is often a simple one, such as the mean or median of the target variable.\n",
    "\n",
    "2. **Sequential Weak Learners:**\n",
    "   - A series of weak learners, typically shallow decision trees (stumps), are sequentially added to the model.\n",
    "   - Each weak learner is trained to correct the errors (residuals) made by the combination of the existing ensemble.\n",
    "\n",
    "3. **Gradient Descent:**\n",
    "   - The algorithm uses a form of gradient descent to minimize the residuals. It calculates the negative gradient of the loss function with respect to the predictions, indicating the direction and magnitude of the errors.\n",
    "   - The weak learner is then fitted to the negative gradient (residuals) of the current predictions.\n",
    "\n",
    "4. **Weighted Combination:**\n",
    "   - The predictions of all weak learners are combined, and each learner's contribution is weighted based on its performance in reducing the residuals.\n",
    "   - The weights are determined by a hyperparameter called the learning rate, which controls the step size during updates.\n",
    "\n",
    "5. **Iterative Process:**\n",
    "   - Steps 2-4 are repeated for a predefined number of iterations or until a specified level of performance is achieved.\n",
    "   - Each new weak learner focuses on the remaining errors of the ensemble, gradually improving the overall model.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - The final prediction is the sum of the initial prediction and the contributions from all the weak learners.\n",
    "\n",
    "The intuition is that, over iterations, the ensemble of weak learners becomes increasingly adept at capturing the patterns in the data and minimizing the errors. By focusing on the residuals of the current predictions, each new weak learner corrects the deficiencies of the existing ensemble, leading to a powerful and accurate predictive model.\n",
    "\n",
    "Gradient Boosting is robust, can handle complex relationships in the data, and is less prone to overfitting due to its sequential nature and the use of weak learners. Popular implementations include XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc339dd-6cff-4d09-a946-ec61a5bc98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6)\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a step-by-step explanation of how this process occurs:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Start with an initial prediction for the target variable. This initial prediction is often the mean or median of the target variable.\n",
    "\n",
    "2. **Calculate Residuals:**\n",
    "   - Calculate the residuals, which are the differences between the actual target values and the current predictions. The residuals represent the errors that the model needs to correct.\n",
    "\n",
    "3. **Fit a Weak Learner to Residuals:**\n",
    "   - Train a weak learner (typically a shallow decision tree, or \"stump\") on the dataset with the goal of predicting the residuals. The weak learner is chosen to capture the patterns in the data that were not adequately captured by the current ensemble.\n",
    "\n",
    "4. **Update Predictions:**\n",
    "   - Update the predictions by adding the weighted predictions from the latest weak learner to the current ensemble. The weight is determined by a hyperparameter called the learning rate, which controls the step size during updates. The learning rate scales the contribution of each weak learner.\n",
    "\n",
    "5. **Repeat Steps 2-4:**\n",
    "   - Iterate the process by calculating new residuals based on the updated predictions and fitting another weak learner to these residuals.\n",
    "   - At each iteration, the new weak learner is focused on correcting the errors made by the combined predictions of the existing ensemble.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - The final prediction is the sum of the initial prediction and the weighted contributions from all the weak learners.\n",
    "\n",
    "The iterative process continues for a predefined number of iterations (controlled by the number of trees in the ensemble) or until a specified level of performance is achieved. The sequential nature of the algorithm, combined with the focus on minimizing residuals at each step, allows Gradient Boosting to gradually improve the model's predictive performance.\n",
    "\n",
    "The final ensemble of weak learners, each contributing to correcting specific errors, forms a strong predictive model that is often more accurate than any individual weak learner. The most common implementations of Gradient Boosting include XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ede735-516f-44db-b514-caf6e5338c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7)\n",
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key concepts and steps in the algorithm. Here's a step-by-step breakdown of the mathematical intuition:\n",
    "\n",
    "1. **Objective Function:**\n",
    "   - Define an objective function that measures the difference between the current predictions and the actual target values. In regression problems, this could be the mean squared error (MSE) or another appropriate loss function.\n",
    "\n",
    "   \\[ L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2 \\]\n",
    "\n",
    "2. **Initialize Model:**\n",
    "   - Start with an initial prediction, often the mean or median of the target variable.\n",
    "\n",
    "   \\[ F_0(x) = \\text{initial prediction} \\]\n",
    "\n",
    "3. **Residuals Calculation:**\n",
    "   - Calculate the residuals, which are the differences between the actual target values and the current predictions.\n",
    "\n",
    "   \\[ r_i = y_i - F_0(x_i) \\]\n",
    "\n",
    "4. **Sequential Weak Learners:**\n",
    "   - For each iteration \\(m = 1, 2, \\ldots, M\\), where \\(M\\) is the number of weak learners (trees) in the ensemble:\n",
    "     - Fit a weak learner (usually a shallow decision tree) \\(h_m(x)\\) to the residuals \\(r_i\\).\n",
    "     - Update the model by adding the weighted predictions from the weak learner to the current model:\n",
    "\n",
    "       \\[ F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x) \\]\n",
    "\n",
    "       where \\(\\nu\\) is the learning rate, a hyperparameter that controls the step size during updates.\n",
    "\n",
    "5. **Objective Function for Each Iteration:**\n",
    "   - At each iteration, the goal is to minimize the objective function with respect to the updated predictions.\n",
    "\n",
    "   \\[ \\text{minimize} \\sum_{i=1}^{N} L(y_i, F_m(x_i)) \\]\n",
    "\n",
    "6. **Gradient Descent:**\n",
    "   - In each iteration, perform a form of gradient descent to minimize the objective function. The negative gradient of the loss function with respect to the predictions provides the direction and magnitude of the errors.\n",
    "\n",
    "   \\[ \\gamma_m = -\\frac{\\partial}{\\partial F_{m-1}(x)} \\sum_{i=1}^{N} L(y_i, F_{m-1}(x_i)) \\]\n",
    "\n",
    "7. **Fit Weak Learner to Negative Gradient:**\n",
    "   - Fit a weak learner \\(h_m(x)\\) to the negative gradient, which effectively captures the patterns in the data that were not well-predicted by the current ensemble.\n",
    "\n",
    "   \\[ h_m(x) = \\text{argmin}_{h} \\sum_{i=1}^{N} [y_i - F_{m-1}(x_i) - h(x_i)]^2 \\]\n",
    "\n",
    "8. **Update Predictions:**\n",
    "   - Update the predictions by adding the weighted predictions from the weak learner to the current ensemble.\n",
    "\n",
    "   \\[ F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x) \\]\n",
    "\n",
    "9. **Final Prediction:**\n",
    "   - The final prediction is the sum of the initial prediction and the weighted contributions from all the weak learners.\n",
    "\n",
    "   \\[ \\hat{y} = F_M(x) \\]\n",
    "\n",
    "By iteratively adding weak learners and adjusting their weights based on the negative gradient of the loss function, Gradient Boosting aims to create a powerful ensemble model that minimizes the overall prediction errors. The learning rate controls the step size during updates, and the ensemble gradually improves its performance over iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
